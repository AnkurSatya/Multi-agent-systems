{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61166986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36e67778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bandit gives a random reward from a particular Gaussian distribution.\n",
    "class Bandit:\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def sample(self):\n",
    "        return np.random.normal(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e213151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmedBandit:\n",
    "    def __init__(self, num_arms=2, means=None, stds=1):\n",
    "        if means is None:\n",
    "            self.means = np.random.uniform(0, 5, num_arms)\n",
    "        else:\n",
    "            self.means = means\n",
    "        if stds is None:\n",
    "            self.stds = np.random.uniform(0, 3, num_arms)\n",
    "        else:\n",
    "            self.stds = stds\n",
    "        self.bandits = [Bandit(mean, std) for mean, std in zip(self.means, self.stds)]\n",
    "        self.arms_pulled = np.zeros(num_arms, dtype=int)\n",
    "        self.arms_rewards = np.zeros(num_arms)\n",
    "        self.arms_rewards_avg = np.zeros(num_arms)\n",
    "        self.num_arms = num_arms\n",
    "        self.actions = np.arange(0, num_arms)\n",
    "        self.epsilon_strategy_used = 0\n",
    "        \n",
    "        self.optimal_action = np.argmax(self.means)\n",
    "        self.optimal_action_reward = np.max(self.means)\n",
    "        self.regrets = []\n",
    "        \n",
    "        self.lai_robbins_values = []\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.__init__(self.num_arms, self.means, self.stds)\n",
    "        \n",
    "    def get_greedy_action(self):\n",
    "        return np.argmax(self.arms_rewards_avg)\n",
    "    \n",
    "    def get_ucb_action(self, c):\n",
    "        t = np.sum(self.arms_pulled)\n",
    "        uncertainty_values = []\n",
    "        for val in self.arms_pulled:\n",
    "            if val == 0:\n",
    "                uncertainty_values.append(float('inf'))\n",
    "            else:\n",
    "                uncertainty_values.append(c*np.sqrt(np.log(t)/val))\n",
    "        return np.argmax(np.array(uncertainty_values))\n",
    "        \n",
    "        \n",
    "    def action_selection(self, strategy=\"epsilon_greedy\", eps=None, c=None):\n",
    "        selected_action = None\n",
    "        \n",
    "        if strategy == \"epsilon_greedy\":\n",
    "            assert eps is not None\n",
    "            greedy_prob = eps + 1.0*(1-eps)/self.num_arms\n",
    "            action_probs = [(1-greedy_prob)/self.num_arms]*self.num_arms\n",
    "            action_probs[self.get_greedy_action()] = greedy_prob\n",
    "            action_probs[-1] += 1.0 - np.sum(action_probs)\n",
    "            \n",
    "            selected_action = np.random.choice(self.actions, p=action_probs)\n",
    "            if selected_action == self.get_greedy_action():\n",
    "                self.epsilon_strategy_used += 1\n",
    "        elif strategy == \"ucb\":\n",
    "            assert c is not None\n",
    "            selected_action = self.get_ucb_action(c)\n",
    "    \n",
    "        return selected_action\n",
    "    \n",
    "    def calculate_lai_robbins_value(self):\n",
    "        t = np.sum(self.arms_pulled)\n",
    "        \n",
    "        lai_robbins_value = 0.0\n",
    "        for i, mean in enumerate(self.means):\n",
    "            if i != self.optimal_action:\n",
    "                diff = self.optimal_action_reward - mean\n",
    "                kl_div = np.log(self.stds[self.optimal_action]/self.stds[i]) + (self.stds[i]**2 + (mean - self.optimal_action_reward)**2)/(2*(self.stds[self.optimal_action]**2)) - 0.5\n",
    "                \n",
    "                lai_robbins_value += (diff/kl_div)\n",
    "        lai_robbins_value *= np.log(t)\n",
    "        self.lai_robbins_values.append(lai_robbins_value)\n",
    "        return lai_robbins_value\n",
    "            \n",
    "        \n",
    "    def sample(self, action):\n",
    "        reward = self.bandits[action].sample()\n",
    "        self.arms_pulled[action] += 1\n",
    "        self.arms_rewards[action] += reward\n",
    "        self.arms_rewards_avg[action] = 1.0*self.arms_rewards[action]/self.arms_pulled[action]\n",
    "        \n",
    "        self.regrets.append(self.optimal_action_reward - self.arms_rewards_avg[action])\n",
    "        \n",
    "        self.calculate_lai_robbins_value()\n",
    "        return reward\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.arms_rewards, self.arms_pulled, self.arms_rewards_avg\n",
    "    \n",
    "    def get_regret_cumsum(self):\n",
    "        return np.cumsum(self.regrets)\n",
    "    \n",
    "    def get_lai_robbins_values(self):\n",
    "        return self.lai_robbins_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1180b40d",
   "metadata": {},
   "source": [
    "## Epsilon greedy approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbfb11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_arms = 5\n",
    "means = np.random.uniform(0, 6, num_arms) \n",
    "stds = [1]*num_arms\n",
    "num_experiments = 100\n",
    "num_iterations = 1000\n",
    "\n",
    "strategy=\"epsilon_greedy\"\n",
    "eps_values = 0.5 + 0.1 * np.arange(0, 5)\n",
    "\n",
    "expected_regret_cumsum_values = dict()\n",
    "expected_lai_robbins_values = dict()\n",
    "correct_arms_pulled = dict()\n",
    "\n",
    "multi_bandit = MultiArmedBandit(num_arms=num_arms, means=means, stds=stds)\n",
    "\n",
    "for eps in eps_values:\n",
    "    all_exps_regret_cumsum = []\n",
    "    all_exps_lai_robbins = []\n",
    "    all_exps_correct_arms_pulled = []\n",
    "    for num_exp in range(1, num_experiments+1):\n",
    "        for i in range(num_iterations):\n",
    "            multi_bandit.sample(multi_bandit.action_selection(strategy=strategy, eps=eps))\n",
    "\n",
    "        all_exps_regret_cumsum.append(multi_bandit.get_regret_cumsum())\n",
    "        all_exps_lai_robbins.append(multi_bandit.get_lai_robbins_values())\n",
    "        \n",
    "        all_exps_correct_arms_pulled.append(round(\n",
    "            100.0*multi_bandit.arms_pulled[multi_bandit.optimal_action]/num_iterations, 2))\n",
    "        multi_bandit.reset()\n",
    "        \n",
    "    all_exps_regret_cumsum = np.array(all_exps_regret_cumsum)\n",
    "    all_exps_lai_robbins = np.array(all_exps_lai_robbins)\n",
    "    \n",
    "    expected_regret_cumsum_values[eps] = np.mean(all_exps_regret_cumsum, axis=0)\n",
    "    expected_lai_robbins_values[eps] = np.mean(all_exps_lai_robbins, axis=0)\n",
    "    correct_arms_pulled[eps] = np.mean(all_exps_correct_arms_pulled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068b684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('epsilon-greedy_regret.pkl', 'wb') as f:\n",
    "    pickle.dump(expected_regret_cumsum_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = correct_arms_pulled.keys(), correct_arms_pulled.values()\n",
    "plt.plot(keys, values)\n",
    "plt.xlabel(\"Epsilon value\")\n",
    "plt.ylabel(\"correct arm pulled(%)\")\n",
    "plt.savefig(\"eps-vs-correct_arm_pulled.jpg\", dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4284f1",
   "metadata": {},
   "source": [
    "## UCB approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9739323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_arms = 5\n",
    "means = np.random.uniform(0, 6, num_arms) \n",
    "stds = [1]*num_arms\n",
    "num_experiments = 100\n",
    "num_iterations = 1000\n",
    "\n",
    "strategy=\"ucb\"\n",
    "c_values = [2.0, 3.0, 5.0, 8.0, 10.0]\n",
    "\n",
    "expected_regret_cumsum_values_ucb = dict()\n",
    "correct_arms_pulled_ucb = dict()\n",
    "\n",
    "multi_bandit = MultiArmedBandit(num_arms=num_arms, means=means, stds=stds)\n",
    "\n",
    "for c in c_values:\n",
    "    all_exps_regret_cumsum = []\n",
    "    all_exps_correct_arms_pulled = []\n",
    "    for num_exp in range(1, num_experiments+1):\n",
    "        for i in range(num_iterations):\n",
    "            multi_bandit.sample(multi_bandit.action_selection(strategy=strategy, c=c))\n",
    "\n",
    "        all_exps_regret_cumsum.append(multi_bandit.get_regret_cumsum())\n",
    "        all_exps_correct_arms_pulled.append(round(\n",
    "            100.0*multi_bandit.arms_pulled[multi_bandit.optimal_action]/num_iterations, 2))\n",
    "        \n",
    "        multi_bandit.reset()\n",
    "        \n",
    "    all_exps_regret_cumsum = np.array(all_exps_regret_cumsum)\n",
    "    expected_regret_cumsum_values_ucb[c] = np.mean(all_exps_regret_cumsum, axis=0)\n",
    "    correct_arms_pulled_ucb[c] = np.mean(all_exps_correct_arms_pulled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9451e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = correct_arms_pulled_ucb.keys(), correct_arms_pulled_ucb.values()\n",
    "plt.plot(keys, values)\n",
    "plt.xlabel(\"c value\")\n",
    "plt.ylabel(\"correct arm pulled(%)\")\n",
    "plt.savefig(\"ucb_c-vs-correct_arm_pulled.jpg\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c4430",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ucb_regret.pkl', 'wb') as f:\n",
    "    pickle.dump(expected_regret_cumsum_values_ucb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde91e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in expected_regret_cumsum_values.items():\n",
    "    plt.plot(np.arange(1, num_iterations+1), value, label=\"eps=\"+str(key))\n",
    "    \n",
    "for key, value in expected_regret_cumsum_values_ucb.items():\n",
    "    plt.plot(np.arange(1, num_iterations+1), value, label=\"c=\"+str(key))\n",
    "\n",
    "plt.plot(np.arange(1, num_iterations+1), expected_lai_robbins_values[0.5], label=\"lr\")\n",
    "    \n",
    "    \n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cummulative regret\")\n",
    "plt.legend()\n",
    "plt.savefig(\"eps_greedy-vs-ucb-vs-lai_robbins.jpg\", dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3f3da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
