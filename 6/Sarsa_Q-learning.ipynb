{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d246a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af6fb1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self, nrows, ncols, pos_start, pos_goal, pos_obstacles, epsilon=0.9, alpha=0.1, \n",
    "                 num_episodes=1, learning_method=\"sarsa\"):\n",
    "        self.nrows = nrows\n",
    "        self.ncols = ncols\n",
    "        self.grid = np.zeros((nrows, ncols))\n",
    "        self.pos_start = pos_start\n",
    "        self.pos_goal = pos_goal\n",
    "        self.pos_obstacles = pos_obstacles\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.num_episodes = num_episodes\n",
    "        self.learning_method=learning_method\n",
    "        \n",
    "        self.UP = 0\n",
    "        self.RIGHT = 1\n",
    "        self.DOWN = 2\n",
    "        self.LEFT = 3\n",
    "        \n",
    "        self.set_grid()\n",
    "        self.Q = self.initialise_q_values()\n",
    "    \n",
    "    def set_grid(self):\n",
    "        self.grid[self.pos_start] = 1\n",
    "        self.grid[self.pos_goal] = 2\n",
    "        self.grid[self.pos_obstacles[:, 0], self.pos_obstacles[:, 1]] = 3\n",
    "        \n",
    "    def initialise_q_values(self):\n",
    "        Q = np.random.rand(nrows, ncols, 4)\n",
    "        Q[self.pos_goal[0], self.pos_goal[1], :] = [0]*4\n",
    "        \n",
    "        for i in range(self.nrows):\n",
    "            for j in range(self.ncols):\n",
    "                available_actions = self.get_available_actions((i, j))\n",
    "                for action in range(4):\n",
    "                    if action not in available_actions:\n",
    "                        Q[i][j][action] = -10**3\n",
    "        \n",
    "        return Q\n",
    "        \n",
    "    def get_available_actions(self, state):\n",
    "        \n",
    "        assert state[0] >=0 and state[0] < self.nrows, \"Row index excedded\"\n",
    "            \n",
    "        assert state[1] >=0 and state[1] < self.ncols, \"Column index excedded\"\n",
    "        \n",
    "        if state[0] == 0:\n",
    "            if state[1] == 0:\n",
    "                return [self.UP, self.RIGHT]\n",
    "            elif state[1] == self.ncols-1:\n",
    "                return [self.UP, self.LEFT]\n",
    "            else:\n",
    "                return [self.UP, self.RIGHT, self.LEFT]\n",
    "        elif state[0] == self.nrows - 1:\n",
    "            if state[1] == 0:\n",
    "                return [self.RIGHT, self.DOWN]\n",
    "            elif state[1] == self.ncols-1:\n",
    "                return [self.DOWN, self.LEFT]\n",
    "            else:\n",
    "                return [self.RIGHT, self.DOWN, self.LEFT]    \n",
    "        elif state[1] == 0:\n",
    "            return [self.UP, self.RIGHT, self.DOWN]\n",
    "        elif state[1] == self.ncols - 1:\n",
    "            return [self.UP, self.DOWN, self.LEFT]\n",
    "        else:\n",
    "            return [self.UP, self.RIGHT, self.DOWN, self.LEFT]        \n",
    "    \n",
    "    def choose_action(self, state, epsilon):\n",
    "        available_actions = self.get_available_actions(state)\n",
    "        while True:\n",
    "            actions = [0, 1, 2, 3]\n",
    "            max_q_value_idx = np.argmax(self.Q[state])\n",
    "\n",
    "            greedy_prob = epsilon + (1.0-epsilon)/len(actions)\n",
    "            rem_prob = (1.0 - greedy_prob)/(len(actions) - 1)\n",
    "\n",
    "            probs = [rem_prob]*len(actions)\n",
    "            probs[max_q_value_idx] = greedy_prob\n",
    "            chosen_action = np.random.choice(actions, p=probs)\n",
    "            \n",
    "            if chosen_action in available_actions:\n",
    "                return chosen_action\n",
    "    \n",
    "    def move(self, state, action):\n",
    "        new_state = None\n",
    "        if action == 0:\n",
    "            new_state = (state[0]+1, state[1])\n",
    "        elif action == 1:\n",
    "            new_state = (state[0], state[1]+1)\n",
    "        elif action == 2:\n",
    "            new_state = (state[0]-1, state[1])\n",
    "        else:\n",
    "            new_state = (state[0], state[1]-1)\n",
    "            \n",
    "        return new_state\n",
    "    \n",
    "    def get_reward(self, new_state):\n",
    "        if self.grid[new_state] == 2:\n",
    "            return 20\n",
    "        elif self.grid[new_state] == 3:\n",
    "            return -100\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def step(self, start_state):\n",
    "        print(\"Learning algorithm: \", self.learning_method)\n",
    "        if self.learning_method== \"sarsa\":\n",
    "            for i in range(self.num_episodes):\n",
    "                state = start_state\n",
    "                action = self.choose_action(state, self.epsilon)\n",
    "                while state != self.pos_goal:\n",
    "                    new_state = self.move(state, action)\n",
    "                    reward = self.get_reward(new_state)\n",
    "\n",
    "                    new_action = self.choose_action(new_state, self.epsilon)\n",
    "\n",
    "                    old_q_val = self.Q[state][action]\n",
    "                    updated_q_val = old_q_val + self.alpha*(reward + self.Q[new_state][new_action] - old_q_val)\n",
    "                    self.Q[state][action] = updated_q_val\n",
    "\n",
    "                    state = new_state\n",
    "                    action = new_action\n",
    "        elif self.learning_method == \"q-learning\":\n",
    "            for i in range(self.num_episodes):\n",
    "                state = start_state\n",
    "                while state != self.pos_goal:\n",
    "                    action = self.choose_action(state, self.epsilon)\n",
    "                    new_state = self.move(state, action)\n",
    "                    reward = self.get_reward(new_state)\n",
    "\n",
    "                    new_action = np.argmax(self.Q[new_state])\n",
    "\n",
    "                    old_q_val = self.Q[state][action]\n",
    "                    updated_q_val = old_q_val + self.alpha*(reward + self.Q[new_state][new_action] - old_q_val)\n",
    "                    self.Q[state][action] = updated_q_val\n",
    "\n",
    "                    state = new_state\n",
    "                \n",
    "        print(\"Learning done!\")\n",
    "        \n",
    "    def plot_grid(self):\n",
    "        grid_plot = np.copy(self.grid)\n",
    "        for i in range(self.nrows):\n",
    "            for j in range(self.ncols):\n",
    "                action = np.argmax(self.Q[i][j])\n",
    "                grid_plot[i][j] = action\n",
    "                \n",
    "        return np.flip(grid_plot, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8063ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 4, 21\n",
    "pos_start = (0,0)\n",
    "pos_goal = (0, 20)\n",
    "pos_obstacles = np.array([(0, i) for i in range(1, ncols-1)])\n",
    "epsilon = 0.9\n",
    "alpha = 0.1\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b9331a",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8de2261e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning done!\n"
     ]
    }
   ],
   "source": [
    "learning_method=\"sarsa\"\n",
    "\n",
    "sarsa_grid = Grid(nrows, ncols, pos_start, pos_goal, pos_obstacles, epsilon=epsilon, alpha=alpha, \n",
    "            num_episodes=num_episodes, learning_method=learning_method)\n",
    "sarsa_grid.step(pos_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8286f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 2. 1. 1. 2. 1. 2. 2. 1. 1. 1. 2. 2. 1. 2. 2. 2. 1. 1. 2.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 2.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(sarsa_grid.plot_grid())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25a3939",
   "metadata": {},
   "source": [
    "## Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f8f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_method=\"q-learning\"\n",
    "\n",
    "q_learning_grid = Grid(nrows, ncols, pos_start, pos_goal, pos_obstacles, epsilon=epsilon, alpha=alpha, \n",
    "            num_episodes=num_episodes, learning_method=learning_method)\n",
    "q_learning_grid.step(pos_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d222dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_learning_grid.plot_grid())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
